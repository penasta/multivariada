---
title: ''
author: ''
date: ''
output:
  pdf_document: null
  fig_crop: no
  html_document:
    df_print: paged
subtitle: ''
highlight: tango
number_sections: no
fig_caption: yes
keep_tex: yes
includes:
  in_header: Estilo.sty
classoption: a4paper
always_allow_html: yes
---
  
  
\begin{center}
{\Large
  DEPARTAMENTO DE ESTATÍSTICA} \\
\vspace{0.5cm}
\begin{figure}[!t]
\centering
\includegraphics[width=9cm, keepaspectratio]{logo-UnB.eps}
\end{figure}
\vskip 1em
{\large
  `r format(Sys.time(), '%d %B %Y')`}
\vskip 3em
{\LARGE
  \textbf{Entrega 6}} \\
\vskip 1em
{\Large
  Prof. Dr. George von Borries} \\
\vskip 1em
{\Large
  Análise Multivariada 1} \\
\vskip 1em
{\Large
  Aluno: Bruno Gondim Toledo | Matrícula: 15/0167636} \\
\vskip 1em
\end{center}

```{r setup, include=F}

if (!require("pacman")) install.packages("pacman")
p_load(knitr,tidyverse,MCMCpack,data.table,psych,GPArotation)

```

\newpage

# Questão 43

## Ex. 9.1 | Johnson & Wichern

Show that the covariance matrix

\[
  \rho =
  \left[ {\begin{array}{ccc}
     1 & .63 & .45\\
     .63 & 1 & .35\\
     .45 & .35 & 1\\
  \end{array} } \right]
\]

for the $p=3$ standardized random variables $Z_1,Z_2,$ and $Z_3$ can be generated by $m=1$ factor model

\[ Z_1 = .9F_1 + \epsilon_1\\
Z_2 = .7F_1 + \epsilon_2\\
Z_3 = .5F_1 + \epsilon_3\]

where $Var(F_1)=1,Cov(\epsilon,F_1)=0$, and

\[
  \Psi = Cov(\epsilon) =
  \left[ {\begin{array}{ccc}
     .19 & 0 & 0\\
     0 & .51 & 0\\
     0 & 0 & .75\\
  \end{array} } \right]
\]

That is, write $\rho$ in the form of $\rho = \mathbf{LL^T+\Psi}.$

### Solução:

```{r q43, include=F}
L <- c(.9,.7,.5)
Lt <- t(L)

L %*% Lt

psi <- matrix(data=c(.19,0,0,
              0,.51,0,
              0,0,.75),3,3)

L %*% Lt + psi
```


Do modelo geral, $\mathbf{X=\mu+LF+\epsilon}$, temos que $\mathbf{L}=[ \ .9 \ \ .7 \ \ .5]$, logo, 
\[
  \mathbf{L^T} =
  \left[ {\begin{array}{ccc}
     .9\\
     .7\\
     .5\\
  \end{array} } \right]
\]
Multiplicando as matrizes $\mathbf{L}$ e $\mathbf{L^T}$, obtemos
\[
  \mathbf{LL^T} =
  \left[ {\begin{array}{ccc}
     .81 & .63 & .45\\
     .63 & .49 & .35\\
     .45 & .35 & .25\\
  \end{array} } \right]
\]
Somando essa matriz a matriz $\mathbf{\Psi}$, obtemos
\[
  \mathbf{LL^T + \Psi} =
  \left[ {\begin{array}{ccc}
     1 & .63 & .45\\
     .63 & 1 & .35\\
     .45 & .35 & 1\\
  \end{array} } \right]
\]
Que é precisamente a matriz $\mathbf{\rho}$, o que demonstra o resultado $\square$

# Questão 44

## Ex. 9.2 | Johnson & Wichern

Use the information in Exercise 9.1.

 (a) Calculate communalities $h_i^2, i=1,2,3$ and interpret these quantities.
 
 (b) Calculate $Corr(Z_i,F_1)$ for $i=1,2,3$. Which variable might carry the greatest weight in "naming" the common factor? Why?
 
### Soluções:
#### a)

As comunalidades são dadas por: $\sum_{j=1}^nl_{ij}^2=h_i^2$. Para a matriz $\mathbf{L}=[.9 \ \ .7 \ \ .5 ]$, temos que as comunalidades são:

$h_1^2=.81\\h_2^2=.49\\h_3^2=.25$.

Como as comunalidades são quantidades de variâncias de cada variável explicada pelos fatores, quanto maior for a comunalidade, maior será o poder de explicação daquela variável pelo fator. A comunalidade $h_i^2$ assume valores no intervalo [0,1]. Desejamos, em geral, valores acima de $0.5$. Neste caso, temos que $h_1^2 > 0.5$, enquanto $h_2^2,h_3^2 < 0.5$. Entretanto $h_2^2 \approx 0.5$, temos que $h_2^2$ também pode ser utilizada.

#### b)

Como $Cov(\mathbf{X,F=L})$, e $Cov(\mathbf{X_i,F_j})=\ell_{}ij$ (Resultado 2., pag. 484 J&W) [1], e $Cov(\mathbf{X_1,F_1}) = Corr(\mathbf{X_1,F_1})$ (pag. 486 J&W) [1] sabemos que $Cor(\mathbf{Z_i,F_1})$, para $i=1,2,3$ será $\ell_{i1}= [.9 \ \ .7 \ \ .5] = \mathbf{L}$. Isso indica que a variável $Z_1$ carrega a maior carga fatorial, dado seu maior valor absoluto (comparando também com o último resultado encontrado sobre comunalidade).

# Questão 45

## Ex. 9.3 | Johnson & Wichern

The eigenvalues and eigenvectors of the correlation matrix $\rho$ in Exercise 9.1 are

$$\begin{aligned}
\lambda_1&=1.96, \qquad  e'_1=[.625, \ .594, \ .507]\\
\lambda_2&= .68, \qquad e'_2=[-.219, \ -.491, \ .843]\\
\lambda_3&= .36, \qquad e'_3=[.749, \ -.638, \ -.177]\\
\end{aligned}
$$ 
(a) Assuming an $m=1$ factor model, calculate the loading matrix $\mathbf{L}$ and matrix of specific variances $\Psi$ using the principal component solution method. Compare the results with those in Exercise 9.1.
 
(b) What proportion of the total population variance is explained by the first common factor?
 
### Soluções

#### a)

Sabemos que $\Sigma = \lambda_1e_1e_1' + \lambda_2e_2e_2' + ... + \lambda_pe_pe'_p = [\sqrt{\lambda_1}e_1 |\sqrt{\lambda_2}e_2 | ... |\sqrt{\lambda_p}e_p] \cdot [\sqrt{\lambda_1}e_1 |\sqrt{\lambda_2}e_2 | ... |\sqrt{\lambda_p}e_p]^T$ (pag. 488 J&W) [1]. Visto que iremos trabalhar com $m=1$, iremos considerar apenas o autovalor $\lambda_1=1.96$ e o autovetor $e'_1=[.625, \ .593, \ .507]$ tal que:

$\sqrt{1.96}[.625 \ \ .593 \ \ .507] = \mathbf{L}$;

e

$\Sigma = \mathbf{LL^T+\Psi}=$

```{r q45a, echo=FALSE}
LL <- c(.625,.594,.507)

LL <- sqrt(1.96) * LL %*% t(LL)

LLT <- t(LL)

SIGMA <- LL %*% LLT

Psi <- matrix(rep(0,9),3,3)

diag(Psi) <- abs(diag(SIGMA)-1)

# Psi

SIGMA <- SIGMA + Psi

SIGMA
```

Onde a matriz $\mathbf{\Psi}$ foi obtida simplesmente somando o valor necessário para a matriz $\mathbf{LL^T}$ ter o valor 1 na daigonal principal, ou seja, $\mathbf{\Psi}=$

```{r q45a2, echo=FALSE}
Psi 
```

#### b)

A proporção da variância explicada pelo primeiro fator comum é $\frac{\lambda_1}{\sum_{i=1}^3\lambda_i}=\frac{1.96}{1.96 + 0.68 + 0.36} = \frac{1.96}{3}\approx$ `r round((1.96/(1.96+.68+.36)),2)`.

# Questão 51

## Ex. 9.19 | Johnson & Wichern

A firm is attempting to evaluate the quality of its sales staff and is trying to find an examination or series of test that may reveal the potential for good performance in sales. The firm has selected a random sample of 50 sales people and has evaluated each on 3 measures of performance: growth of sales, profitability of sales, and new-account sales. These measures have been converted to a scale, on which 100 indicates "average" performance. Each of the 50 individuals took each 4 test, which purported to measure creativity, mechanical reasoning, abstract reasoning, and mathematical ability, respectively. The $n=50$ observations on $p=7$ variables are listed in Table 9.12.

 (a) Assume an orthogonal factor model for the standardized variables $Z_i=\frac{(X_i-\mu_i)}{\sqrt{\sigma_{ii}}},i=1,2,...,7$. Obtain either the principal component solution or the maximum likehood solution for $m=2$ and $m=3$ common factors.
 
 (b) Given your solution in (a), obtain the rotated loadings for $m=2$ and $m=3$. Compare the two sets of rotated loadings. Interpret the $m=2$ and $m=3$ factor solutions.
 
 (c) List the estimated communalities, specific variances, and $\hat{\mathbf{L}}\hat{\mathbf{L}^T}+\hat{\mathbf{\Psi}}$ for the $m=2$ and $m=3$ solutions. Compare the results. Which choice of $m$ do you prefer at this point? Why?
 
 (d) Conduct a test of $H_0:\mathbf{\Sigma=LL'+\Psi}$ versus $H_1:\mathbf{\Sigma \neq LL'+\Psi}$ for both $m=2$ and $m=3$ at the $\alpha=.01$ level. With these results and those in Parts b and c, which choice of $m$ appears to be the best?
 
 (e) Suppose a new salesperson, selected at random, obtains the test scores $\mathbf{x'}=[x_1,x_2,...,x_7]=[110,98,105,15,18,12,35]$. Calculate the salesperson's factor score using the weighted least squares method and the regression method.
 
### Soluções

```{r q51, include=FALSE}
p_load(readr)
dados <- read_table("dados/table9_12-SalespeopleData.DAT.txt", 
    col_names = FALSE)
```


#### a)

##### Solução por componentes principais para m = 2:

```{r q51a, echo=FALSE}
q51a1 <- principal(dados, nfactors = 2, rotate = 'none',
                 covar = FALSE)
q51a1
load1 <- q51a1$loadings[,1:2]
plot(load1,type="n")
text(load1,labels=c("X1","X2","X3",
                   "X4","X5","X6","X7"),cex=1)
```

##### Solução por máxima verossimilhança para m = 3:

```{r q51a2, echo=F}
q51a2 <- factanal(covmat = cor(dados),
                        factors = 3, rotation = "none")
q51a2
```

#### b)

##### Rotação varimax na solução por componentes principais para m = 2:

```{r q51b, echo=FALSE}
q51a3 <- principal(dados, nfactors = 2, rotate = 'varimax',
                 covar = FALSE)
q51a3
load2 <- q51a1$loadings[,1:2]
```

##### Rotação quartimax na solução por máxima verossimilhança para m = 3:

```{r q51b2, echo=F}
q51a4 <- factanal(covmat = cor(dados),
                        factors = 3, rotation = "quartimax")
q51a4
```

##### Interpretação:
No caso da solução para $m=2$, foi utilizada a rotação varimax que busca explicitar a relação entre os fatores, enquanto na solução para $m=3$ optei por utilizar a rotação quartimax, que busca minimizar o número de fatores necessários para explicar a variável. Em ambos os casos, notamos que a variância acumulada na primeira carga aumenta, em especial para a rotação quartimax para $m=3$. Como assumimos a ortogonalidade do modelo ao início do problema, optamos por essas duas rotações ortogonais. Como sabemos, a rotação da base serve para encontrar a posição de maior variância explicada pela elipsoide projetada no plano; não alterando portanto a informação contida nos dados.

#### c) 

##### Solução: Para permitir a comparabilidade, o ideal agora é solucionar pelo mesmo método. Irei optar por componentes principais em ambos os casos.

```{r q51c, include=FALSE}
q51a5 <- principal(dados, nfactors = 3, rotate = 'none',
                 covar = FALSE)
q51a1
q51a5

```

Para $m=2$, temos as comunalidades $h_i^2\approx[.95 \ .88 \ .89 \ .85 \ .69 \ .80 \ .87]$, variância específica $[.04 \ .11 \ .10 \ .14 \ .30 \ .19 \ .12]$ e $\hat{\mathbf{L}}\hat{\mathbf{L}}+\hat{\mathbf{\Psi}}$ = 

```{r q51c1, echo=F}
psi511 <- matrix(rep(0,49),7,7)
diag(psi511) <- q51a1[["uniquenesses"]]
q51a1$loadings[,1:2] %*% t(q51a1$loadings[,1:2]) + psi511
```


Enquanto para $m=3$ temos as comunalidades $h_i^2 = [.96 \ .98 \ .91 \ .95 \ .69 \ .98 \ .96]$, variância específica $[.03 \ .01 \ .08 \ .04 \ .30 \ .01 \ .03]$ e $\hat{\mathbf{L}}\hat{\mathbf{L}}+\hat{\mathbf{\Psi}}$ =

```{r q51c2, echo=F}
psi512 <- matrix(rep(0,49),7,7)
diag(psi512) <- q51a5[["uniquenesses"]]
q51a5$loadings[,1:3] %*% t(q51a5$loadings[,1:3]) + psi512
```

Se compararmos com a matriz $\rho$ de variância-covariância:

```{r q51c3, echo=FALSE}
cor(dados)
```

Neste caso, notamos que com $m=2$ já é possível obter $\approx85\%$ da variância, o que me parece ser um valor bem razoável pelo tamanho da redução. Portanto, eu optaria pela solução com $m=2$ para problemas não tão conservadores, em que é satisfatório este valor. Talvez se fosse um estudo muito crítico e com necessidade de ser conservador, pudesse ser considerada a solução para $m=3$.

#### d)

```{r,eval=FALSE}
cor(dados)
q51a5$loadings[,1:3] %*% t(q51a5$loadings[,1:3]) + psi512
q51a1$loadings[,1:2] %*% t(q51a1$loadings[,1:2]) + psi511

cortest(R1=cor(dados),
        R2=q51a5$loadings[,1:3] %*% t(q51a5$loadings[,1:3]) + psi512,
        n1=50,n2 = 50)

cortest(R1=cor(dados),
        q51a1$loadings[,1:2] %*% t(q51a1$loadings[,1:2]) + psi511,
        n1=50,n2 = 50)
p_load(covTestR)
cor.test(cor(dados), q51a5$loadings[,1:3] %*% t(q51a5$loadings[,1:3]) + psi512)

cor.test(cor(dados), q51a1$loadings[,1:2] %*% t(q51a1$loadings[,1:2]) + psi511)

compareCov(cor(dados), q51a1$loadings[,1:2] %*% t(q51a1$loadings[,1:2]) + psi511)

```


#### e)

##### Solução

Pelo método de mínimos quadrados ponderados, para o caso da solução $m=2$, temos que $\mathbf{\hat{f_j}=(\hat{L'}\hat{\Psi}^{-1}\hat{L})^{-1}\hat{L'}\hat{\Psi}^{-1}(x_j-\hat{\mu})}$ (pag. 515 J&W) [1]. Portanto, teremos:
```{r q51e1}
# m=2
# Seja L = 

L <- q51a1$loadings[,1:2]

# Então L' será

LT <- t(L)

# Já a matriz Psi, é dada por:

PSI <- matrix(rep(0,49),7,7)

diag(PSI) <- q51a1[["uniquenesses"]] 

# E seja o vetor de médias de x1,...,x7=

XB <- c(mean(dados$X1),mean(dados$X2),mean(dados$X3),mean(dados$X4),
        mean(dados$X5),mean(dados$X6),mean(dados$X7))

# e seja o vetor xj:

XJ <- c(110,98,105,15,18,12,35)

# então f_j^chapeu será

FJH <-(solve(LT %*% solve(PSI) %*% L) %*% LT %*% solve(PSI))*(XB-XJ)
FJH
```



\newpage

# Referências:

## [1] JOHNSON, Richard A; WICHERN, Dean W. APPLIED MULTIVARIATE STATISTICAL ANALYSIS. 6ª Edição. Pearson, 2007.

## [2] von Borries, George. Material de aula disponível no Aprender3; Notas de aula e códigos. Análise Multivariada 1. Universidade de Brasília, 2023.